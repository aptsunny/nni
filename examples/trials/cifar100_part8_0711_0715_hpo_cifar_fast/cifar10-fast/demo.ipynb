{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU clocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-persistenced failed to initialize. Check syslog for more details.\n",
      "Applications clocks set to \"(MEM 877, SM 1530)\" for GPU 00000000:00:1E.0\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "!sudo nvidia-persistenced\n",
    "!sudo nvidia-smi -ac 877,1530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import *\n",
    "from torch_backend import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(c_in, c_out):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': BatchNorm(c_out), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def residual(c):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c),\n",
    "        'res2': conv_bn(c, c),\n",
    "        'add': (Add(), ['in', 'res2/relu']),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3')):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    n = {\n",
    "        'input': (None, []),\n",
    "        'prep': conv_bn(3, channels['prep']),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1']), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2']), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3']), pool=pool),\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'logits': Mul(weight),\n",
    "    }\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer])\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dc458e0ef54dab99669ea73edecff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Preprocessing training data\n",
      "\n",
      "Finished in 2.3 seconds\n",
      "Preprocessing test data\n",
      "Finished in 0.092 seconds\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "dataset = cifar10(root=DATA_DIR)\n",
    "timer = Timer()\n",
    "print('Preprocessing training data')\n",
    "transforms = [\n",
    "    partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "    partial(transpose, source='NHWC', target='NCHW'), \n",
    "]\n",
    "train_set = list(zip(*preprocess(dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "print(f'Finished in {timer():.2} seconds')\n",
    "print('Preprocessing test data')\n",
    "valid_set = list(zip(*preprocess(dataset['valid'], transforms).values()))\n",
    "print(f'Finished in {timer():.2} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = ColorMap()\n",
    "draw = lambda graph: DotGraph({p: ({'fillcolor': colors[type(v)], 'tooltip': repr(v)}, inputs) for p, (v, inputs) in graph.items() if v is not None})\n",
    "\n",
    "draw(build_graph(net()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "NB: on the first run, the first epoch will be slower as initialisation and Cudnn benchmarking take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run 0 at 2020-07-10 13:26:56\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       6.0708       1.6451       0.4086       1.1962       1.5527       0.4384       6.0708\n",
      "           2       6.4397       0.9469       0.6606       0.4630       0.8451       0.7067       6.4397\n",
      "           3       6.4559       0.7288       0.7444       0.4552       0.8143       0.7286       6.4559\n",
      "           4       6.4892       0.6332       0.7803       0.3594       0.6937       0.7590       6.4892\n",
      "           5       4.1490       0.5606       0.8039       0.2180       0.9481       0.6717       4.1490\n",
      "           6       6.6454       0.5010       0.8277       0.4661       0.7351       0.7564       6.6454\n",
      "           7       6.4345       0.4443       0.8480       0.4597       0.4807       0.8335       6.4345\n",
      "           8       6.4906       0.4137       0.8582       0.4915       0.6213       0.7955       6.4906\n",
      "           9       4.6319       0.3873       0.8683       0.2177       0.4708       0.8415       4.6319\n",
      "          10       5.4079       0.3646       0.8746       0.4655       0.5432       0.8166       5.4079\n",
      "          11       6.4606       0.3437       0.8820       0.4709       0.4972       0.8341       6.4606\n",
      "          12       6.4452       0.3271       0.8888       0.4571       0.4534       0.8431       6.4452\n",
      "          13       5.2689       0.3089       0.8955       0.3821       0.3920       0.8739       5.2689\n",
      "          14       4.9163       0.2877       0.9037       0.4484       0.3666       0.8760       4.9163\n",
      "          15       6.4468       0.2695       0.9088       0.4569       0.3876       0.8705       6.4468\n",
      "          16       6.5041       0.2490       0.9161       0.4519       0.5075       0.8370       6.5041\n",
      "          17       5.6404       0.2355       0.9210       0.2145       0.2983       0.9007       5.6404\n",
      "          18       4.3455       0.2083       0.9298       0.6062       0.3856       0.8789       4.3455\n",
      "          19       6.4682       0.1934       0.9353       0.4663       0.2625       0.9126       6.4682\n",
      "          20       6.4885       0.1655       0.9454       0.4714       0.2609       0.9151       6.4885\n",
      "          21       6.5166       0.1437       0.9534       0.4515       0.2647       0.9122       6.5166\n",
      "          22       4.2327       0.1163       0.9632       0.2177       0.2098       0.9298       4.2327\n",
      "          23       5.9529       0.0915       0.9722       0.4619       0.1973       0.9339       5.9529\n",
      "          24       6.4406       0.0753       0.9780       0.4703       0.1815       0.9387       6.4406\n",
      "Starting Run 1 at 2020-07-10 13:29:34\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       6.4489       1.6541       0.4009       0.4681       1.2867       0.5365       6.4489\n",
      "           2       5.2213       0.9474       0.6624       0.2180       0.8551       0.7051       5.2213\n",
      "           3       4.3935       0.7237       0.7468       0.4702       0.8538       0.7219       4.3935\n",
      "           4       6.4860       0.6371       0.7798       0.4529       0.8303       0.7350       6.4860\n",
      "           5       6.4357       0.5590       0.8063       0.4711       0.5725       0.8004       6.4357\n",
      "           6       5.8495       0.4963       0.8299       0.2184       0.7242       0.7525       5.8495\n",
      "           7       4.0858       0.4416       0.8489       0.2178       0.5007       0.8219       4.0858\n",
      "           8       6.7297       0.4113       0.8584       0.4627       0.7239       0.7568       6.7297\n",
      "           9       6.4566       0.3788       0.8708       0.4592       0.4776       0.8337       6.4566\n",
      "          10       6.5084       0.3632       0.8766       0.5002       0.4762       0.8393       6.5084\n",
      "          11       4.5107       0.3461       0.8827       0.2172       0.4139       0.8616       4.5107\n",
      "          12       5.7267       0.3251       0.8904       0.4698       0.4949       0.8338       5.7267\n",
      "          13       6.4925       0.3066       0.8962       0.4643       0.4165       0.8639       6.4925\n",
      "          14       6.4853       0.2891       0.9024       0.4651       0.4132       0.8586       6.4853\n",
      "          15       4.9887       0.2709       0.9085       0.3829       0.3917       0.8693       4.9887\n",
      "          16       4.9357       0.2543       0.9140       0.4695       0.3996       0.8661       4.9357\n",
      "          17       6.4615       0.2328       0.9221       0.4743       0.3236       0.8942       6.4615\n",
      "          18       6.4570       0.2077       0.9308       0.4741       0.3475       0.8828       6.4570\n",
      "          19       5.5731       0.1888       0.9371       0.2152       0.2828       0.9043       5.5731\n",
      "          20       4.7785       0.1642       0.9462       0.4869       0.2636       0.9117       4.7785\n",
      "          21       6.4561       0.1394       0.9554       0.4655       0.2783       0.9085       6.4561\n",
      "          22       6.4437       0.1178       0.9639       0.4656       0.2238       0.9248       6.4437\n",
      "          23       6.3640       0.0925       0.9723       0.2150       0.1852       0.9379       6.3640\n",
      "          24       4.2681       0.0754       0.9788       0.2185       0.1784       0.9387       4.2681\n",
      "Starting Run 2 at 2020-07-10 13:32:02\n",
      "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       6.2193       1.6528       0.3989       0.4605       1.4007       0.5283       6.2193\n",
      "           2       6.4488       0.9456       0.6627       0.4642       1.3211       0.6019       6.4488\n",
      "           3       6.4979       0.7416       0.7387       0.5031       0.9359       0.7000       6.4979\n",
      "           4       5.0748       0.6259       0.7842       0.2155       0.7641       0.7370       5.0748\n",
      "           5       4.4824       0.5665       0.8019       0.4687       0.6668       0.7657       4.4824\n",
      "           6       6.4827       0.4999       0.8291       0.4733       0.4654       0.8423       6.4827\n",
      "           7       6.4886       0.4406       0.8499       0.4783       0.4840       0.8423       6.4886\n",
      "           8       5.6913       0.4094       0.8593       0.3935       0.5266       0.8184       5.6913\n",
      "           9       4.7308       0.3818       0.8681       0.4699       0.5677       0.8058       4.7308\n",
      "          10       6.4804       0.3669       0.8746       0.4608       0.4239       0.8563       6.4804\n",
      "          11       6.4697       0.3433       0.8835       0.4698       0.4000       0.8676       6.4697\n",
      "          12       6.3377       0.3246       0.8896       0.2665       0.3930       0.8631       6.3377\n",
      "          13       4.0477       0.3089       0.8961       0.2179       0.7940       0.7515       4.0477\n",
      "          14       6.5954       0.2876       0.9020       0.4665       0.5321       0.8204       6.5954\n",
      "          15       6.4631       0.2710       0.9094       0.4708       0.3572       0.8805       6.4631\n",
      "          16       6.4894       0.2509       0.9149       0.4527       0.3643       0.8788       6.4894\n",
      "          17       4.6286       0.2302       0.9230       0.2153       0.3110       0.8950       4.6286\n",
      "          18       5.4780       0.2071       0.9309       0.4635       0.2912       0.9013       5.4780\n",
      "          19       6.4887       0.1860       0.9384       0.4569       0.3067       0.8989       6.4887\n",
      "          20       6.4112       0.1626       0.9464       0.4616       0.2472       0.9194       6.4112\n",
      "          21       4.7652       0.1386       0.9551       0.3729       0.2280       0.9233       4.7652\n",
      "          22       4.5263       0.1161       0.9633       0.4717       0.2260       0.9267       4.5263\n",
      "          23       6.4798       0.0945       0.9714       0.4565       0.1953       0.9351       6.4798\n",
      "          24       6.4629       0.0755       0.9787       0.4706       0.1779       0.9424       6.4629\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.9424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_acc  valid_acc\n",
       "count   1.000000     1.0000\n",
       "mean    0.978677     0.9424\n",
       "std          NaN        NaN\n",
       "min     0.978677     0.9424\n",
       "25%     0.978677     0.9424\n",
       "50%     0.978677     0.9424\n",
       "75%     0.978677     0.9424\n",
       "max     0.978677     0.9424"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=24\n",
    "lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "train_transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "N_runs = 3\n",
    "\n",
    "train_batches = DataLoader(Transform(train_set, train_transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)\n",
    "valid_batches = DataLoader(valid_set, batch_size, shuffle=False, drop_last=False)\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "\n",
    "summaries = []\n",
    "for i in range(N_runs):\n",
    "    print(f'Starting Run {i} at {localtime()}')\n",
    "    model = Network(net()).to(device).half()\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(epochs):\n",
    "        logs.append(union({'epoch': epoch+1}, train_epoch(state, Timer(torch.cuda.synchronize), train_batches, valid_batches)))\n",
    "logs.df().query(f'epoch=={epochs}')[['train_acc', 'valid_acc']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
